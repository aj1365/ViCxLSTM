{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e725e6a8-5de7-4cd4-b021-5822bf8e26ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, Subset, ConcatDataset\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import ImageFolder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "937d6c43-ba12-486d-8b25-19a154c812a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e95464-36cd-416b-bd0a-cf7b70acde06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a91f511a-71a7-4bee-8cd3-4abc5a44ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'E:/Dataset/NWPURESISC_45/'\n",
    "\n",
    "TRAIN_DIR = DATA_DIR + 'train'\n",
    "TEST_DIR = DATA_DIR + 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5935138-4fd6-4f22-bf24-35a07dba70cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = os.listdir(TRAIN_DIR)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dec6da-a5e9-4f76-ae8b-e7d2e81d7b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TRAIN')\n",
    "for label in classes:\n",
    "    files = os.listdir('{}/{}'.format(TRAIN_DIR, label))\n",
    "    print('{} {} \\t {}'.format(len(files), label, files[:3]))\n",
    "\n",
    "print()\n",
    "print('TEST')\n",
    "for label in classes:\n",
    "    files = os.listdir('{}/{}'.format(TEST_DIR, label))\n",
    "    print('{} {} \\t {}'.format(len(files), label, files[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "475c4316-08c6-4e25-8679-b4529c71efac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1491, 609)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.transforms as T\n",
    "def _my_normalization(x):\n",
    "    return x + (0.01**0.5)*torch.randn(x.shape)\n",
    "\n",
    "\n",
    "transform = T.Compose([\n",
    "T.Pad(16), # Use 8 or 16 for 256 data\n",
    "T.RandomRotation([-5,5]), # Start here, increas\n",
    "T.Resize(64),\n",
    "T.RandomCrop(64),\n",
    "#T.ColorJitter(brightness=0.5,contrast=0.5, saturation=0.5),    \n",
    "T.ToTensor(),\n",
    "#T.Lambda(_my_normalization),\n",
    "         \n",
    "#T.Normalize(mean=[0.36801723, 0.3809769, 0.34357962], std=[0.20345809, 0.18542756, 0.18488906], inplace=False),\n",
    "#T.NoiseInjection(p=0.5, mean=0, std=0.5), # Write a nn.Module that adds\n",
    "# randn_like to data. Suggest giving it a weight. Forward is\n",
    "# data = data if torch.rand(1) > self.p else data + self.weight * (self.mean + (self.std * torch.randn_like(data)))\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(TRAIN_DIR, transform=transform)\n",
    "test_dataset = ImageFolder(TEST_DIR, transform=transform)\n",
    "\n",
    "\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7931381-bdd5-437a-9bb5-7828a28a58dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d22cdca3-38b5-49f6-abb9-db2acdf9adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(img, label):\n",
    "    plt.title('Label: {}, ({})'.format(classes[label], label))\n",
    "    plt.imshow(img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e826ca2-28c9-4f80-9760-1ae590423356",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = random.randrange(len(train_dataset))\n",
    "show_sample(*train_dataset[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2888744c-f82f-4f14-9b5a-8b13c692fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = random.randrange(len(train_dataset))\n",
    "show_sample(*train_dataset[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc020567-f2e8-46b9-9011-1975b08f4eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x249052d8650>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a2de3093-3099-4d7d-8c62-3029cd11aab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1491, 609)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subset = train_dataset\n",
    "test_subset = test_dataset\n",
    "len(train_subset), len(test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a91c1629-e925-4aae-b3eb-d26babc39150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1491, 182, 427)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = int(len(test_subset) * 0.3)\n",
    "test_size = len(test_subset) - val_size\n",
    "\n",
    "test_subset, val_subset = random_split(test_subset, [test_size, val_size])\n",
    "len(train_subset), len(val_subset), len(test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f426d8ad-7456-472d-a5a2-dab1c638434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a805676d-5855-420b-91b4-f0ffa911f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_subset, batch_size, shuffle=True, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_subset, batch_size,  pin_memory=True)\n",
    "test_dataloader = DataLoader(test_subset, batch_size, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b67353-ecd6-447a-886b-8cd5c467b318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ca57c406-8747-436f-a6aa-c51e8f3330ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dl, invert=False):\n",
    "    for images, labels in dl:\n",
    "        fig, ax = plt.subplots(figsize=(16, 16))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        data = 1-images if invert else images\n",
    "        ax.imshow(make_grid(images[:32], nrow=8).permute(1, 2, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b0cb1-5d02-40b1-a723-71ad6c5b19b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6a7521-26d9-48e2-9013-2931825d9fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fa848715-9aa6-4a26-96d6-68103c15f929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b4d6a9-adb3-43ac-a829-4334097d32d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "253e614a-c5a9-4679-aed2-8a0d04bbee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67606640-9906-4275-9d0b-0d7b52c41541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(torch.cuda.get_device_name())\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07018a8-fbb6-4f91-8c9f-39f5717a2934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc3ed89-b01c-4cee-9ac9-d06c51937f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "from vision_lstm3 import VisionLSTM2\n",
    "\n",
    "model = VisionLSTM2(\n",
    "    dim=192,\n",
    "    input_shape=(3, 64, 64),\n",
    "    patch_size=4,\n",
    "    depth=2,\n",
    "    output_shape=(45,),\n",
    "    mode=\"classifier\",\n",
    "    pooling=\"bilateral_flatten\",\n",
    "    drop_path_rate=0.1,\n",
    "    drop_path_decay=True,\n",
    "    stride=None,\n",
    "    legacy_norm=False,\n",
    "    conv_kind=\"2d\",\n",
    "    conv_kernel_size=3,\n",
    "    proj_bias=True,\n",
    "    norm_bias=True\n",
    "    feature_extractor_channels=[32, 64, 128, 192],  # Example channel sizes for each layer\n",
    "    use_fourier=True  # Enable wavelet transform\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5ead9d-fb09-45d7-95eb-fcaf0fef8c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fc7160bf-babc-438d-8b14-1ff903851534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CosineScheduler import CosineScheduler\n",
    "lr = 0.001\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.5)\n",
    "\n",
    "nepochs = 30\n",
    "warmup=5\n",
    "linear_wi=True\n",
    "scheduler = CosineScheduler(optimizer,total_epochs=nepochs,\n",
    "                            warmup=warmup,\n",
    "                            linear_wu=linear_wi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb2784e-0b14-4e48-a2dc-7ef09124c3b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7abc58-8a84-4385-aff9-7563cf57f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "scaler = GradScaler()  # Initialize the gradient scaler for mixed precision\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def Train(epoch, print_every=30):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time()\n",
    "    \n",
    "    accuracy = []\n",
    "    \n",
    "    for i, batch in enumerate(train_dataloader, 1):\n",
    "        minput = batch[0]  # Get batch of images from our train dataloader\n",
    "        target = batch[1]  # Get the corresponding target\n",
    "        minput, target = minput.to(device), target.to(device)\n",
    "        \n",
    "        moutput = model(minput)  # output by our model\n",
    "        \n",
    "        loss = criterion(moutput, target)  # compute cross-entropy loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()  # Clear the gradients\n",
    "        loss.backward()  # Backpropagate the loss\n",
    "        optimizer.step()  # Update Model parameters\n",
    "        scheduler.step()\n",
    "        \n",
    "        argmax = moutput.argmax(dim=1)  # Get the class index with maximum probability predicted by the model\n",
    "        accuracy.append((target == argmax).sum().item() / target.shape[0])  # calculate accuracy\n",
    "\n",
    "        if i % print_every == 0:\n",
    "            print(f'Epoch: [{epoch}/{num_epochs}], Step: [{i}/{len(train_dataloader)}], '\n",
    "                  f'Train Loss: {loss.item():.4f}, Accuracy: {sum(accuracy)/len(accuracy):.2f}, '\n",
    "                  f'Time: {time() - start_time:.2f} sec')\n",
    "    \n",
    "    avg_train_accuracy = sum(accuracy) / len(accuracy) if accuracy else 0\n",
    "    return total_loss / len(train_dataloader), avg_train_accuracy  # Returning Average Training Loss and Accuracy\n",
    "\n",
    "def Test(epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    start_time = time()\n",
    "\n",
    "    accuracy = []\n",
    "    \n",
    "    with torch.inference_mode():  # Disable gradient calculations\n",
    "        for i, batch in enumerate(val_dataloader):\n",
    "            minput = batch[0]  # Get batch of images from our test dataloader\n",
    "            target = batch[1]  # Get the corresponding target\n",
    "            minput, target = minput.to(device), target.to(device)\n",
    "            \n",
    "            moutput = model(minput)  # output by our model\n",
    "\n",
    "            loss = criterion(moutput, target)  # compute cross-entropy loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            argmax = moutput.argmax(dim=1)  # Find the class with maximum score\n",
    "            accuracy.append((target == argmax).sum().item() / target.shape[0])  # Calculate accuracy\n",
    "            \n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "    avg_accuracy = sum(accuracy) / len(accuracy) if accuracy else 0\n",
    "    \n",
    "    print(f'Epoch: [{epoch}/{num_epochs}], Test Loss: {avg_loss:.4f}, '\n",
    "          f'Accuracy: {avg_accuracy:.2f}, Time: {time() - start_time:.2f} sec')\n",
    "    \n",
    "    return avg_loss, avg_accuracy  # Returning Average Testing Loss and Accuracy\n",
    "\n",
    "# Initialize variables for tracking the best model\n",
    "best_loss = float('inf')\n",
    "best_model_wts = None\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "num_epochs = 50  # Set the number of epochs\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_accuracy = Train(epoch, print_every=10)\n",
    "    test_loss, test_accuracy = Test(epoch)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    # Check if the current model is the best so far\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        best_model_wts = model.state_dict()  # Save the best model's weights\n",
    "    \n",
    "    torch.cuda.empty_cache()  # Clear cached memory\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "# Save the best model after all epochs are completed\n",
    "if best_model_wts is not None:\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model.state_dict(), 'VixLSTM_best_model_UCMerced.pth')\n",
    "    print(f'Best model saved with loss: {best_loss:.4f}')\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a25dcc-2fc0-40e0-867a-6181be5a2243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e44a87d2-690f-4026-ac64-7c45af15f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predictions(model, dataloader):\n",
    "    torch.cuda.empty_cache()\n",
    "    preds_list = torch.zeros(0, dtype=torch.long, device=device)\n",
    "    labels_list = torch.zeros(0, dtype=torch.long, device=device)\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds  = torch.max(outputs, dim=1)\n",
    "        preds_list = torch.cat((preds_list, preds), 0)\n",
    "        labels_list = torch.cat((labels_list, labels), 0)\n",
    "    return preds_list, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ebdf47bb-863d-4868-a525-c260ddb11992",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list, labels_list = predictions(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7444c916-23a0-4178-abbb-497ca36a45a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4fcdb902-8041-4806-b820-1b3c7824b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,recall_score,cohen_kappa_score,accuracy_score\n",
    "\n",
    "preds_list, labels_list = predictions(model, test_dataloader)\n",
    "preds_list, labels_list\n",
    "\n",
    "SAVE_PATH='E:/Results/'\n",
    "\n",
    "## classfication report\n",
    "test_pred = preds_list.cpu()\n",
    "test_true = labels_list.cpu()\n",
    "\n",
    "OA = accuracy_score(test_true,test_pred)\n",
    "AA = recall_score(test_true,test_pred,average='macro')\n",
    "kappa = cohen_kappa_score(test_true,test_pred)\n",
    "report_log = F\"OA: {OA}\\nAA: {AA}\\nKappa: {kappa}\\n\"\n",
    "report_log += classification_report(test_true,test_pred,target_names=classes,digits=4)\n",
    "print(report_log)\n",
    "fp = open(os.path.join(SAVE_PATH,'ViCxLSTM_classfication_report_NWPURESISC_45.txt'),'w+')\n",
    "fp.writelines(report_log)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beea4b31-6a00-41c9-ae68-04cec4084687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
